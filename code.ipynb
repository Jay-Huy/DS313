{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec9f6b9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d840a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tuleh\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\tuleh\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(168997376, 168997760)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from asr_model import ASRModel\n",
    "\n",
    "# a, b, c = ASRModel(model_dim=768, mode = 'A').to('cuda'), ASRModel(model_dim=768, mode = 'B').to('cuda'), ASRModel(model_dim=768, mode = 'C').to('cuda')\n",
    "# a.params, b.params, c.params\n",
    "\n",
    "from asr_model import ASRModel\n",
    "\n",
    "a_last6 = ASRModel(model_dim=768, mode = 'A', layer_selection_mode='last6').to('cuda')\n",
    "a_last3_first3 = ASRModel(model_dim=768, mode = 'A', layer_selection_mode='first3_last3').to('cuda')\n",
    "# b = ASRModel(model_dim=768, mode = 'B').to('cuda')\n",
    "a_last6.params, a_last3_first3.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4f9fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 35]), torch.Size([10, 20, 10240]), torch.Size([10, 35]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "audio_features = torch.randn(10, 20, 10240).to('cuda')  # Example input tensor (seq_len, batch_size, acoustic_input_dim)\n",
    "input_ids = torch.randint(0, a_last6.vocab_size, (10, 35)).long().to('cuda')\n",
    "attention_mask = torch.ones(10, 35).to('cuda')  # Example attention mask (batch_size, seq_len)\n",
    "input_ids.shape, audio_features.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d8bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 35, 32128])\n"
     ]
    }
   ],
   "source": [
    "a_lala = a_last6(input_ids = input_ids,\n",
    "           attention_mask = attention_mask,\n",
    "           audio_features = audio_features)\n",
    "print(a_lala.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d369a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 35, 32128])\n"
     ]
    }
   ],
   "source": [
    "a_lele = a_last3_first3(input_ids = input_ids,\n",
    "           attention_mask = attention_mask,\n",
    "           audio_features = audio_features)\n",
    "print(a_lele.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1242fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18709, 24352, 27565,  1301, 27287, 27853, 19539, 14865,  1713,  1098,\n",
       "         25399, 27336,  8245,  6407,  5206, 17804, 23071, 28113,  9647, 22601,\n",
       "         10010, 20818, 29141, 16834, 15952, 12004, 13890, 15369, 16810, 23345,\n",
       "          6498,  5475, 28467, 19619, 12016],\n",
       "        [27062, 14089,  4540, 25402,  8996, 18154,  2325, 27679,   793,  1259,\n",
       "         12432, 17898,  7090, 18738, 26590,  4858, 13563, 26742, 14417,  7151,\n",
       "         17575, 25988, 26449,  2034,  7990,  6789,  3799, 27871,  8362,  3642,\n",
       "         21410, 24595, 16655, 10161, 14096],\n",
       "        [19218, 27976, 23676, 26617,  5807,  8588, 28033, 11417,  3006,  4410,\n",
       "         20807, 30994, 17011,  7036, 10534, 16325,  7222, 27857, 27184, 14556,\n",
       "          4415,  3076, 15633,  8371, 27896, 17479,  5869, 28508, 23160, 10998,\n",
       "         22800, 22789, 25967,  5484, 11911],\n",
       "        [28865,  5025, 10183, 27720, 18769, 27122, 27683, 23721,  6896, 29061,\n",
       "          1889, 21086, 11837, 21362,  8240, 28411,  5730, 28528, 19615, 20859,\n",
       "         31243,  4608, 19906, 19109, 25452,  7300, 18244, 27718, 25572, 18734,\n",
       "         23220, 18134, 11995, 29398, 20361],\n",
       "        [ 2966,  4496, 21293, 23473, 28420,  8655, 17603, 19871, 21990, 20069,\n",
       "         24789, 20449, 13238, 13006,  2575, 19239, 13810, 13252,  4628, 24507,\n",
       "         11639, 20454, 31470, 21455, 21351, 16079, 28664,  1885, 17436,  7389,\n",
       "         11799,  5915, 30393,  2180, 11947],\n",
       "        [11131, 29142, 27656,  8306,  4984, 27533,  5680, 18433, 16040,  2611,\n",
       "          5229,  7673,   812,  4576, 17242, 22796, 11283,  5760,   917, 25819,\n",
       "         26106, 16131, 30420,  9553, 28178, 25558, 11805, 17533, 17401, 12874,\n",
       "         31958,  9791, 21118,  7683, 25308],\n",
       "        [14199, 30420, 18130, 16213,  8081,  2462,  6439, 28523,  1930,  1063,\n",
       "         22252,  2496, 31030, 27649, 22524, 27356, 27876,  3963, 19861, 26142,\n",
       "         22789, 28606, 28317, 13103, 14977, 24918, 19397, 12232, 18833, 26519,\n",
       "         24221,  5240, 22124, 22713,  2989],\n",
       "        [11262, 27642,  2650, 20332,   721, 26028, 22454, 18984, 12713, 25227,\n",
       "          3014, 27791, 24658,  6480, 23666,  1387, 27545,  7103, 19200,  2124,\n",
       "          4412, 14725, 26293, 23259, 19774, 29960,  6461,  8313, 30663,  3683,\n",
       "          6936, 20744, 26215, 11521,  7185],\n",
       "        [30548,  2116, 18972, 15569, 25172,  7027, 28577, 29100,  3121, 18367,\n",
       "         32127, 12830,  7139,   782, 26534, 21413, 23976,  3040,  4301,  7464,\n",
       "         28019, 19705, 22308, 28637, 21377, 24346, 14548, 22106,  6538,  5671,\n",
       "          6526,  2286,  8830,  6237, 24494],\n",
       "        [ 7194, 24595, 30207,  8989, 14593,  7252,  5602, 22789, 24862, 11976,\n",
       "          9311, 27532, 17364,  1296, 32127,  7107, 28901, 28113, 23595, 26679,\n",
       "         19939,  8061,  7090, 12092, 19677, 25053, 10404, 25066, 17967, 15061,\n",
       "         14119, 20198, 28373, 29874, 30469]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hihi = a_lala.argmax(dim=-1)\n",
    "hihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03f7e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "a[:-6], a[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f260e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 35, 32128])\n"
     ]
    }
   ],
   "source": [
    "b_lala = b(input_ids = input_ids,\n",
    "           attention_mask = None,\n",
    "           audio_features = audio_features)\n",
    "print(b_lala.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e2f1023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 35, 21128])\n"
     ]
    }
   ],
   "source": [
    "c_lala = c(input_ids = input_ids,\n",
    "           attention_mask = None,\n",
    "           audio_features = audio_features)\n",
    "print(c_lala.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27a06d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Decoder' object has no attribute '_shift_right'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma_last6\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_shift_right\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tuleh\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Decoder' object has no attribute '_shift_right'"
     ]
    }
   ],
   "source": [
    "a_last6.decoder._shift_right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcc1be",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf515f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\tuleh\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def _shift_right(input_ids, decoder_start_token_id, pad_token_id):\n",
    "    \"\"\"\n",
    "    Shifts the input_ids to the right and prepends the decoder_start_token_id.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): Tensor of shape (batch_size, seq_length) containing input token IDs.\n",
    "        decoder_start_token_id (int): The token ID to prepend at the start of each sequence.\n",
    "        pad_token_id (int): The token ID used for padding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (batch_size, seq_length) with shifted input IDs.\n",
    "    \"\"\"\n",
    "    if decoder_start_token_id is None:\n",
    "        raise ValueError(\n",
    "            \"decoder_start_token_id must be defined. In T5, it is usually set to the pad_token_id.\"\n",
    "        )\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"pad_token_id must be defined.\")\n",
    "\n",
    "    # Create a tensor for the shifted input IDs\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "\n",
    "    # Shift inputs to the right\n",
    "    shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n",
    "\n",
    "    # Replace the first token with the decoder_start_token_id\n",
    "    shifted_input_ids[..., 0] = decoder_start_token_id\n",
    "\n",
    "    # Replace possible -100 values in labels with `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from asr_model import ASRModel\n",
    "from evaluate import load\n",
    "cer = load(\"cer\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Langboat/mengzi-t5-base\")\n",
    "a_last6 = ASRModel(model_dim=768, mode = 'A', layer_selection_mode='last6').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3dc4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: ['而 对 楼市 成交 抑制 作用 最 大 的 限 购', '也 成为 地方 政府 的 眼中 钉', '自 六月 底 呼和浩特 市 率先 宣布 取消 限 购 后', '各地 政府 便 纷纷 跟进']\n",
      "Text used as ground-truth: ['而 对 楼市 成交 抑制 作用 最 大 的 限 购</s>', '也 成为 地方 政府 的 眼中 钉</s><pad><pad><pad><pad><pad><pad><pad><pad>', '自 六月 底 呼和浩特 市 率先 宣布 取消 限 购 后</s>', '各地 政府 便 纷纷 跟进</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n",
      "Text used as training data: ['<pad>而 对 楼市 成交 抑制 作用 最 大 的 限 购', '<pad> 也 成为 地方 政府 的 眼中 钉</s><pad><pad><pad><pad><pad><pad><pad>', '<pad>自 六月 底 呼和浩特 市 率先 宣布 取消 限 购 后', '<pad> 各地 政府 便 纷纷 跟进</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenzier\n",
    "# bos_token = tokenizer.cls_token_id\n",
    "# eos_token = tokenizer.sep_token_id\n",
    "# pad_token = tokenizer.pad_token_id\n",
    "\n",
    "# Example text input and features\n",
    "text = [\"而 对 楼市 成交 抑制 作用 最 大 的 限 购\", \"也 成为 地方 政府 的 眼中 钉\", \"自 六月 底 呼和浩特 市 率先 宣布 取消 限 购 后\", \"各地 政府 便 纷纷 跟进\"]\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length = 256)\n",
    "decoded_text = tokenizer.batch_decode(input_ids[\"input_ids\"])\n",
    "decoder_start_token_id = a_last6.decoder.config.decoder_start_token_id\n",
    "shifted_input_ids = _shift_right(input_ids[\"input_ids\"], decoder_start_token_id, tokenizer.pad_token_id)\n",
    "shifted_text = tokenizer.batch_decode(shifted_input_ids)\n",
    "print(f'Original text: {text}')\n",
    "print(f'Text used as ground-truth: {decoded_text}')\n",
    "print(f'Text used as training data: {shifted_text}')\n",
    "\n",
    "downsampled_features = torch.rand(4, 128, 205, 80) # (B, C, T, F) -> (B, T, C*F)\n",
    "batch = {\n",
    "    'input_ids': input_ids.input_ids,\n",
    "    'attention_mask': input_ids.attention_mask,\n",
    "    'downsampled_features': downsampled_features,\n",
    "}\n",
    "for k in batch:\n",
    "    batch[k] = batch[k].to(device=device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38009c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  34,    7,  263,    7,  967,    7,  619,    7,    5,    7, 7315,    7,\n",
       "        8741,    1,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[\"input_ids\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e05b77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[\"attention_mask\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4ad836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  36,    7,   27,    7, 8169,    7, 3583,    7, 4915,    7,  759,    7,\n",
       "          126,    7,   28,    7,    5,    7, 4020,    7, 3572,    1],\n",
       "        [  34,    7,  263,    7,  967,    7,  619,    7,    5,    7, 7315,    7,\n",
       "         8741,    1,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.input_ids[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d54b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>而 对 楼市 成交 抑制 作用 最 大 的 限 购',\n",
       " '<pad> 也 成为 地方 政府 的 眼中 钉<pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<pad>自 六月 底 呼和浩特 市 率先 宣布 取消 限 购 后',\n",
       " '<pad> 各地 政府 便 纷纷 跟进<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(shifted_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36df058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 0], ['</s>', '<unk>', '<pad>'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids[:3], tokenizer.all_special_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed3cfe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"T5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"t5\",\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_last6.decoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "044e99cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5LayerSelfAttention(\n",
       "  (SelfAttention): T5Attention(\n",
       "    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.block[0].layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d011a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875a72f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5LayerNorm()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.final_layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f62863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['而 对 楼 市 成 交 抑 制 作 用 最 大 的 限 购 [SEP] [PAD] [PAD] [PAD] [PAD]',\n",
       " '也 成 为 地 方 政 府 的 眼 中 钉 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '自 六 月 底 呼 和 浩 特 市 率 先 宣 布 取 消 限 购 后 [SEP] [PAD]',\n",
       " '各 地 政 府 便 纷 纷 跟 进 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = batch['input_ids']  # Batch_size, seq_length\n",
    "downsampled_features = batch['downsampled_features']  # Batch_size, seq_length, feature_dim\n",
    "attention_mask = batch['attention_mask']  # Batch_size, seq_length\n",
    "\n",
    "shifted_left_outputs = torch.cat([input_ids[:, 1:], torch.full((input_ids.size(0), 1), tokenizer.pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
    "tokenizer.batch_decode(shifted_left_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74310df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 21128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, C, T, F = downsampled_features.shape\n",
    "outputs = model(input_ids, attention_mask, downsampled_features.view(B, T, C*F))  # Batch_size, seq_length, vocab_size\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d689cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CER and loss\n",
    "ids_prediction = outputs.argmax(dim=-1)  # Batch_size, seq_length\n",
    "predictions = tokenizer.batch_decode(ids_prediction, skip_special_tokens=True)  # Batch_size\n",
    "references = tokenizer.batch_decode(shifted_left_outputs, skip_special_tokens=True)  # Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5d97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References: ['而 对 楼 市 成 交 抑 制 作 用 最 大 的 限 购', '也 成 为 地 方 政 府 的 眼 中 钉', '自 六 月 底 呼 和 浩 特 市 率 先 宣 布 取 消 限 购 后', '各 地 政 府 便 纷 纷 跟 进']\n"
     ]
    }
   ],
   "source": [
    "print(f'References: {references}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b8160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['ex 闌 脳 筑晝還綑弈詡 鲑 a灬 helpapp 驍端 笆窗 錳 间ien', '##餡澆 蚤箍奘兰 蜆過崛 辻 畳med 1907锏锏 喟 焘 による 铿 第', '##しています 鈕 fx娄鋅 汶 繩 xddd丐 害将 惘撮訶のは realᅣvr 畳yo', '##琳尻 贻 蒞 捍 co2 淌 电槓 饰骁 飘炼 剣靴淼 hard 间 庠挾']\n"
     ]
    }
   ],
   "source": [
    "print(f'Predictions: {predictions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_score = cer.compute(predictions=predictions, references=references)\n",
    "cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "051ba5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 20, 21128]), torch.Size([4, 20]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, shifted_left_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd99dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huy\\AppData\\Local\\Temp\\ipykernel_16536\\4162233661.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cer_score = torch.tensor(cer_score, dtype=torch.float32, device=outputs.device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.0796, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(outputs, labels, cer_score, criterion, gamma=1.0, ignore_index=0):\n",
    "    \"\"\"\n",
    "    Compute the weighted loss for the Decoder using alpha weights based on CER.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions of shape (batch_size, seq_length, vocab_size).\n",
    "        labels (torch.Tensor): Ground truth labels of shape (batch_size, seq_length).\n",
    "        cer_score (float): Character Error Rate (CER) for the Decoder.\n",
    "        gamma (float): Hyperparameter to control the influence of CER in alpha computation.\n",
    "        ignore_index (int): The padding value in the labels to be ignored in the loss calculation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The computed weighted loss.\n",
    "    \"\"\"\n",
    "    # Ensure cer_score is a tensor\n",
    "    cer_score = torch.tensor(cer_score, dtype=torch.float32, device=outputs.device)\n",
    "\n",
    "    # Compute alpha weight based on CER\n",
    "    alpha = -(cer_score ** gamma) * torch.log(1 - cer_score + 1e-8)  # Add epsilon to avoid log(0)\n",
    "\n",
    "    # Compute the base loss\n",
    "    base_loss = criterion(outputs.mT, labels)  # Flatten outputs and labels\n",
    "    if base_loss < 1:\n",
    "        try: \n",
    "            alpha = -(cer_score ** gamma) * torch.log(1 - cer_score + 1e-8)  # Add epsilon to avoid log(0)\n",
    "            base_loss = base_loss * alpha  # Apply alpha weight to the loss\n",
    "        except:\n",
    "            base_loss = base_loss # Avoid cer_score is bigger than 1\n",
    "\n",
    "    return base_loss\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_token)\n",
    "loss = loss_fn(outputs, shifted_left_outputs, cer_score, criterion)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
